{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qfFD671aExog"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with building the linearized voltage dynmiacs model."
      ],
      "metadata": {
        "id": "sYUwbI5tOshh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the adjecent matrix for IEEE 13-bus sytem (except the root bus):\n",
        "A = -np.eye(12)\n",
        "A[1,0]=1\n",
        "A[2,0]=1\n",
        "A[3,0]=1\n",
        "A[4,1]=1\n",
        "A[5,2]=1\n",
        "A[6,2]=1\n",
        "A[7,2]=1\n",
        "A[8,3]=1\n",
        "A[9,5]=1\n",
        "A[10,5]=1\n",
        "A[11,7]=1\n",
        "# how this adjecent matrix is built:\n",
        "# lines(rows):0-1,1-2,1-5,1-3,2-4,5-7,5-8,5-9,3-6,7-10,7-11,9-12\n",
        "# buses (columns):1,2,5,3,4,7,8,9,6,10,11,12\n",
        "\n",
        "# Impedence values for each lines\n",
        "X = np.diag([0.3856,0.1276,0.3856,0.1119,0.0765,0.0771,0.1928,0.0423,0.1119,0.0766,0.0766,0.0423])\n",
        "\n",
        "F = -np.linalg.inv(A)\n",
        "X =2*F@X@F.T\n",
        "# Thus the linearized power flow equation is\n",
        "# v = Xq + v_{env}, note here v is squared voltage magniture."
      ],
      "metadata": {
        "id": "MAjh58pkOpuO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data generation"
      ],
      "metadata": {
        "id": "ARtIwlroPo8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(np.linalg.eigvals(X))\n",
        "# X is a positive definite matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKKnhyFUP5Cx",
        "outputId": "b84b9825-d005-45e3-8cee-50c6c3dc8bd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.025732389209492255"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obviously, v = Xq + v_{env} is monotone w.r.t. q\n",
        "# For simplicity, we set v_{env} to one.\n",
        "q_data = (np.random.rand(12,10000).astype(np.float32) - 0.5)*0.2 #range [-0.1,0.1]\n",
        "v_data = X@q_data + 1\n",
        "print(v_data.T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-b1kS-Pj_-",
        "outputId": "e3d5ee1c-e6ae-487a-fc81-87bbc6ec0756"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the monotone models"
      ],
      "metadata": {
        "id": "pNoftFreTU-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monotone Neural Network, Codes from paper 'Monotone, Bi-Lipschitz, and Polyak-{\\L}ojasiewicz Networks'\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import Sequence, Callable\n",
        "\n",
        "def cayley(W: torch.Tensor) -> torch.Tensor:\n",
        "    cout, cin = W.shape\n",
        "    if cin > cout:\n",
        "        return cayley(W.T).T\n",
        "    U, V = W[:cin, :], W[cin:, :]\n",
        "    I = torch.eye(cin, dtype=W.dtype, device=W.device)\n",
        "    A = U - U.T + V.T @ V\n",
        "    iIpA = torch.inverse(I + A)\n",
        "    return torch.cat((iIpA @ (I - A), -2 * V @ iIpA), axis=0)\n",
        "\n",
        "class MonLipNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 features: int,\n",
        "                 unit_features: Sequence[int],\n",
        "                 mu: float = 0.1, #lower bound for monotonicity\n",
        "                 nu: float = 10., #upper bound for monotonicity\n",
        "                 act_fn: Callable = None):\n",
        "        super().__init__()\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        self.device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        self.mu = mu\n",
        "        self.nu = nu\n",
        "        self.units = unit_features\n",
        "        self.act_fn = act_fn\n",
        "        self.Fq = nn.Parameter(torch.empty(sum(self.units), features)).to(self.device)\n",
        "        nn.init.xavier_normal_(self.Fq)\n",
        "        self.fq = nn.Parameter(torch.empty((1,))).to(self.device)\n",
        "        nn.init.constant_(self.fq, self.Fq.norm())\n",
        "        self.by = nn.Parameter(torch.zeros(features)).to(self.device)\n",
        "        Fr, fr, b = [], [], []\n",
        "        if act_fn is not None:\n",
        "            d = []\n",
        "        nz_1 = 0\n",
        "        for nz in self.units:\n",
        "            R = nn.Parameter(torch.empty((nz, nz+nz_1))).to(self.device)\n",
        "            nn.init.xavier_normal_(R)\n",
        "            r = nn.Parameter(torch.empty((1,))).to(self.device)\n",
        "            nn.init.constant_(r, R.norm())\n",
        "            Fr.append(R)\n",
        "            fr.append(r)\n",
        "            b.append(nn.Parameter(torch.zeros(nz)))\n",
        "            if act_fn is not None:\n",
        "                d.append(nn.Parameter(torch.zeros(nz)).to(self.device))\n",
        "            nz_1 = nz\n",
        "        self.Fr = nn.ParameterList(Fr).to(self.device)\n",
        "        self.fr = nn.ParameterList(fr).to(self.device)\n",
        "        self.b = nn.ParameterList(b).to(self.device)\n",
        "        if act_fn is not None:\n",
        "            self.d = nn.ParameterList(d)\n",
        "        # cached weights\n",
        "        self.Q = None\n",
        "        self.R = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        sqrt_gam = math.sqrt(self.nu - self.mu)\n",
        "        sqrt_2 = math.sqrt(2.)\n",
        "        if self.training:\n",
        "            self.Q, self.R = None, None\n",
        "            Q = cayley(self.fq * self.Fq / self.Fq.norm()).to(self.device)\n",
        "            R = [cayley(fr * Fr / Fr.norm()) for Fr, fr in zip(self.Fr, self.fr)]\n",
        "        else:\n",
        "            if self.Q is None:\n",
        "                with torch.no_grad():\n",
        "                    self.Q = cayley(self.fq * self.Fq / self.Fq.norm())\n",
        "                    self.R = [cayley(fr * Fr / Fr.norm()) for Fr, fr in zip(self.Fr, self.fr)]\n",
        "            Q, R = self.Q, self.R\n",
        "\n",
        "        xh = sqrt_gam * x.to(self.device) @ Q.T\n",
        "        yh = []\n",
        "        hk_1 = xh[..., :0]\n",
        "        idx = 0\n",
        "        for k, nz in enumerate(self.units):\n",
        "            xk = xh[..., idx:idx+nz]\n",
        "            gh = torch.cat((xk, hk_1), dim=-1).to(self.device)\n",
        "            if self.act_fn is None:\n",
        "                gh = sqrt_2 * F.relu(sqrt_2 * gh @ R[k].T + self.b[k]) @ R[k]\n",
        "            else:\n",
        "                gh = sqrt_2 * (gh @ R[k].T) + self.b[k]\n",
        "                gh = self.act_fn(gh * torch.exp(self.d[k])) * torch.exp(-self.d[k])\n",
        "                gh = sqrt_2 * gh @ R[k]\n",
        "            hk = gh[..., :nz] - xk\n",
        "            gk = gh[..., nz:]\n",
        "            yh.append(hk_1-gk)\n",
        "            idx += nz\n",
        "            hk_1 = hk\n",
        "        yh.append(hk_1)\n",
        "\n",
        "        yh = torch.cat(yh, dim=-1).to(self.device)\n",
        "        y = 0.5 * ((self.mu + self.nu) * x.to(self.device) + sqrt_gam * yh @ Q) + self.by\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "gbfhq6vHTUOn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ICNN\n",
        "class ReHU(nn.Module):\n",
        "    \"\"\" Rectified Huber unit\"\"\"\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.a = 1/d\n",
        "        self.b = -d/2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.clamp(torch.sign(x)*self.a/2*x**2,min=0,max=-self.b),x+self.b)\n",
        "\n",
        "\n",
        "class ICNN(nn.Module):\n",
        "    def __init__(self, layer_sizes, activation=F.relu_):\n",
        "        super().__init__()\n",
        "        self.W = nn.ParameterList([nn.Parameter(torch.Tensor(l, layer_sizes[0]))\n",
        "                                   for l in layer_sizes[1:]])\n",
        "        self.U = nn.ParameterList([nn.Parameter(torch.Tensor(layer_sizes[i+1], layer_sizes[i]))\n",
        "                                   for i in range(1,len(layer_sizes)-1)])\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(l)) for l in layer_sizes[1:]])\n",
        "        self.act = activation\n",
        "        self.reset_parameters()\n",
        "        self.rehu = ReHU(1.0)\n",
        "        self.act2 = torch.nn.Tanh()\n",
        "        self.beta = nn.Parameter(torch.tensor(5.0), requires_grad=False)\n",
        "        self.act_sp = torch.nn.Softplus()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # copying from PyTorch Linear\n",
        "        for W in self.W:\n",
        "            nn.init.kaiming_uniform_(W, a=0.1**0.5)\n",
        "        for U in self.U:\n",
        "            nn.init.kaiming_uniform_(U, a=0.1**0.5)\n",
        "        for i,b in enumerate(self.bias):\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W[i])\n",
        "            bound = 1 / (fan_in**0.5)\n",
        "            nn.init.uniform_(b, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        beta = torch.abs(self.beta)\n",
        "        z = F.linear(x, self.W[0], self.bias[0])\n",
        "        z = self.act_sp(z*beta)/beta\n",
        "\n",
        "        for W,b,U in zip(self.W[1:-1], self.bias[1:-1], self.U[:-1]):\n",
        "            z = F.linear(x, W, b) + F.linear(z, F.softplus(U)) / U.shape[0]\n",
        "            z = self.act_sp(z*beta)/beta\n",
        "\n",
        "        V_res = F.linear(x, self.W[-1], self.bias[-1]) + F.linear(z, F.softplus(self.U[-1])) / self.U[-1].shape[0]\n",
        "        return V_res * 0.1 #here 0.1 is only for scalaring purpose\n",
        "\n",
        "class Mono_ICNN_grad(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_dim,distribued=False):\n",
        "        super(Mono_ICNN_grad, self).__init__()\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        self.device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "        self.rehu = ReHU(float(7.0))\n",
        "        self.icnn = ICNN([obs_dim, hidden_dim, hidden_dim, 1], activation=torch.nn.Softplus())\n",
        "        self.distributed = distribued\n",
        "\n",
        "    def forward(self, state):\n",
        "        output = self.icnn(state)\n",
        "        compute_batch_jacobian = torch.vmap(torch.func.jacrev(self.icnn))\n",
        "        y = compute_batch_jacobian(state).squeeze()\n",
        "        return y"
      ],
      "metadata": {
        "id": "b93Le_tFRXGm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_NNs():\n",
        "    features = 12\n",
        "    units = [200, 200]\n",
        "    mu, nu = 0.01, 20\n",
        "    FTN_net = MonLipNet(features, units, mu, nu)\n",
        "    FTN_optimizer = torch.optim.Adam(FTN_net.parameters(), lr=1e-3)\n",
        "\n",
        "    # define the monotone neural network with gradient of SCNN\n",
        "    hidden_dim = 200\n",
        "    cvx_net = Mono_ICNN_grad(12, hidden_dim)\n",
        "\n",
        "    # params_to_optimize = [param for g in policy_net.g_list for param in g.parameters()]\n",
        "    cvx_optimizer = torch.optim.Adam(cvx_net.parameters(), lr=1e-3)\n",
        "    # Loss function\n",
        "    criterion = nn.MSELoss()\n",
        "    return FTN_net, FTN_optimizer, cvx_net,cvx_optimizer,criterion"
      ],
      "metadata": {
        "id": "GCt5q7WoVMya"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 100\n",
        "v_data = np.float32(v_data)\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(q_data.T), torch.tensor(v_data.T))\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "FTN_net, FTN_optimizer, cvx_net,cvx_optimizer,criterion = init_NNs()\n",
        "\n",
        "loss_cvx_list = []\n",
        "loss_ftn_list = []\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    cvx_epoch_loss = 0\n",
        "    for i, (q, v) in enumerate(train_dataloader):\n",
        "        # Reset gradients\n",
        "        FTN_optimizer.zero_grad()\n",
        "        cvx_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute predicted actions by passing states to the model\n",
        "        predicted_v = FTN_net(q)\n",
        "        cvx_predicted_v = cvx_net(q)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(predicted_v, v)\n",
        "        cvx_loss = criterion(cvx_predicted_v, v)\n",
        "\n",
        "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        cvx_loss.backward()\n",
        "\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        FTN_optimizer.step()\n",
        "        cvx_optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        epoch_loss += loss.item()\n",
        "        cvx_epoch_loss += cvx_loss.item()\n",
        "    loss_cvx_list.append(cvx_epoch_loss)\n",
        "    loss_ftn_list.append(epoch_loss)\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f\"****FTN method****(Linear): Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/i:.5f}\")\n",
        "    print(f\"****CVX method****(Linear): Epoch {epoch+1}/{epochs}, Loss: {cvx_epoch_loss/i:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd_O0f6NWFC1",
        "outputId": "5376f064-fc6c-4919-f8d2-485158eee9dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****FTN method****(Linear): Epoch 1/100, Loss: 0.36568\n",
            "****CVX method****(Linear): Epoch 1/100, Loss: 0.81256\n",
            "****FTN method****(Linear): Epoch 2/100, Loss: 0.02676\n",
            "****CVX method****(Linear): Epoch 2/100, Loss: 0.22075\n",
            "****FTN method****(Linear): Epoch 3/100, Loss: 0.00577\n",
            "****CVX method****(Linear): Epoch 3/100, Loss: 0.06378\n",
            "****FTN method****(Linear): Epoch 4/100, Loss: 0.00303\n",
            "****CVX method****(Linear): Epoch 4/100, Loss: 0.04729\n",
            "****FTN method****(Linear): Epoch 5/100, Loss: 0.00170\n",
            "****CVX method****(Linear): Epoch 5/100, Loss: 0.04555\n",
            "****FTN method****(Linear): Epoch 6/100, Loss: 0.00108\n",
            "****CVX method****(Linear): Epoch 6/100, Loss: 0.04391\n",
            "****FTN method****(Linear): Epoch 7/100, Loss: 0.00070\n",
            "****CVX method****(Linear): Epoch 7/100, Loss: 0.04212\n",
            "****FTN method****(Linear): Epoch 8/100, Loss: 0.00047\n",
            "****CVX method****(Linear): Epoch 8/100, Loss: 0.04097\n",
            "****FTN method****(Linear): Epoch 9/100, Loss: 0.00034\n",
            "****CVX method****(Linear): Epoch 9/100, Loss: 0.03889\n",
            "****FTN method****(Linear): Epoch 10/100, Loss: 0.00025\n",
            "****CVX method****(Linear): Epoch 10/100, Loss: 0.03738\n",
            "****FTN method****(Linear): Epoch 11/100, Loss: 0.00021\n",
            "****CVX method****(Linear): Epoch 11/100, Loss: 0.03531\n",
            "****FTN method****(Linear): Epoch 12/100, Loss: 0.00015\n",
            "****CVX method****(Linear): Epoch 12/100, Loss: 0.03374\n",
            "****FTN method****(Linear): Epoch 13/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 13/100, Loss: 0.03221\n",
            "****FTN method****(Linear): Epoch 14/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 14/100, Loss: 0.03041\n",
            "****FTN method****(Linear): Epoch 15/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 15/100, Loss: 0.02859\n",
            "****FTN method****(Linear): Epoch 16/100, Loss: 0.00008\n",
            "****CVX method****(Linear): Epoch 16/100, Loss: 0.02694\n",
            "****FTN method****(Linear): Epoch 17/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 17/100, Loss: 0.02528\n",
            "****FTN method****(Linear): Epoch 18/100, Loss: 0.00006\n",
            "****CVX method****(Linear): Epoch 18/100, Loss: 0.02364\n",
            "****FTN method****(Linear): Epoch 19/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 19/100, Loss: 0.02205\n",
            "****FTN method****(Linear): Epoch 20/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 20/100, Loss: 0.02042\n",
            "****FTN method****(Linear): Epoch 21/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 21/100, Loss: 0.01896\n",
            "****FTN method****(Linear): Epoch 22/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 22/100, Loss: 0.01767\n",
            "****FTN method****(Linear): Epoch 23/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 23/100, Loss: 0.01623\n",
            "****FTN method****(Linear): Epoch 24/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 24/100, Loss: 0.01498\n",
            "****FTN method****(Linear): Epoch 25/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 25/100, Loss: 0.01382\n",
            "****FTN method****(Linear): Epoch 26/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 26/100, Loss: 0.01279\n",
            "****FTN method****(Linear): Epoch 27/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 27/100, Loss: 0.01181\n",
            "****FTN method****(Linear): Epoch 28/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 28/100, Loss: 0.01083\n",
            "****FTN method****(Linear): Epoch 29/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 29/100, Loss: 0.01002\n",
            "****FTN method****(Linear): Epoch 30/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 30/100, Loss: 0.00927\n",
            "****FTN method****(Linear): Epoch 31/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 31/100, Loss: 0.00861\n",
            "****FTN method****(Linear): Epoch 32/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 32/100, Loss: 0.00800\n",
            "****FTN method****(Linear): Epoch 33/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 33/100, Loss: 0.00745\n",
            "****FTN method****(Linear): Epoch 34/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 34/100, Loss: 0.00694\n",
            "****FTN method****(Linear): Epoch 35/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 35/100, Loss: 0.00649\n",
            "****FTN method****(Linear): Epoch 36/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 36/100, Loss: 0.00607\n",
            "****FTN method****(Linear): Epoch 37/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 37/100, Loss: 0.00570\n",
            "****FTN method****(Linear): Epoch 38/100, Loss: 0.00006\n",
            "****CVX method****(Linear): Epoch 38/100, Loss: 0.00536\n",
            "****FTN method****(Linear): Epoch 39/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 39/100, Loss: 0.00507\n",
            "****FTN method****(Linear): Epoch 40/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 40/100, Loss: 0.00478\n",
            "****FTN method****(Linear): Epoch 41/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 41/100, Loss: 0.00450\n",
            "****FTN method****(Linear): Epoch 42/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 42/100, Loss: 0.00427\n",
            "****FTN method****(Linear): Epoch 43/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 43/100, Loss: 0.00405\n",
            "****FTN method****(Linear): Epoch 44/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 44/100, Loss: 0.00385\n",
            "****FTN method****(Linear): Epoch 45/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 45/100, Loss: 0.00363\n",
            "****FTN method****(Linear): Epoch 46/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 46/100, Loss: 0.00345\n",
            "****FTN method****(Linear): Epoch 47/100, Loss: 0.00008\n",
            "****CVX method****(Linear): Epoch 47/100, Loss: 0.00328\n",
            "****FTN method****(Linear): Epoch 48/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 48/100, Loss: 0.00313\n",
            "****FTN method****(Linear): Epoch 49/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 49/100, Loss: 0.00298\n",
            "****FTN method****(Linear): Epoch 50/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 50/100, Loss: 0.00284\n",
            "****FTN method****(Linear): Epoch 51/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 51/100, Loss: 0.00270\n",
            "****FTN method****(Linear): Epoch 52/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 52/100, Loss: 0.00256\n",
            "****FTN method****(Linear): Epoch 53/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 53/100, Loss: 0.00245\n",
            "****FTN method****(Linear): Epoch 54/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 54/100, Loss: 0.00234\n",
            "****FTN method****(Linear): Epoch 55/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 55/100, Loss: 0.00224\n",
            "****FTN method****(Linear): Epoch 56/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 56/100, Loss: 0.00213\n",
            "****FTN method****(Linear): Epoch 57/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 57/100, Loss: 0.00203\n",
            "****FTN method****(Linear): Epoch 58/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 58/100, Loss: 0.00195\n",
            "****FTN method****(Linear): Epoch 59/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 59/100, Loss: 0.00185\n",
            "****FTN method****(Linear): Epoch 60/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 60/100, Loss: 0.00178\n",
            "****FTN method****(Linear): Epoch 61/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 61/100, Loss: 0.00170\n",
            "****FTN method****(Linear): Epoch 62/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 62/100, Loss: 0.00163\n",
            "****FTN method****(Linear): Epoch 63/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 63/100, Loss: 0.00156\n",
            "****FTN method****(Linear): Epoch 64/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 64/100, Loss: 0.00149\n",
            "****FTN method****(Linear): Epoch 65/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 65/100, Loss: 0.00143\n",
            "****FTN method****(Linear): Epoch 66/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 66/100, Loss: 0.00137\n",
            "****FTN method****(Linear): Epoch 67/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 67/100, Loss: 0.00132\n",
            "****FTN method****(Linear): Epoch 68/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 68/100, Loss: 0.00127\n",
            "****FTN method****(Linear): Epoch 69/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 69/100, Loss: 0.00121\n",
            "****FTN method****(Linear): Epoch 70/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 70/100, Loss: 0.00116\n",
            "****FTN method****(Linear): Epoch 71/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 71/100, Loss: 0.00111\n",
            "****FTN method****(Linear): Epoch 72/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 72/100, Loss: 0.00107\n",
            "****FTN method****(Linear): Epoch 73/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 73/100, Loss: 0.00103\n",
            "****FTN method****(Linear): Epoch 74/100, Loss: 0.00001\n",
            "****CVX method****(Linear): Epoch 74/100, Loss: 0.00098\n",
            "****FTN method****(Linear): Epoch 75/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 75/100, Loss: 0.00095\n",
            "****FTN method****(Linear): Epoch 76/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 76/100, Loss: 0.00091\n",
            "****FTN method****(Linear): Epoch 77/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 77/100, Loss: 0.00087\n",
            "****FTN method****(Linear): Epoch 78/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 78/100, Loss: 0.00084\n",
            "****FTN method****(Linear): Epoch 79/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 79/100, Loss: 0.00081\n",
            "****FTN method****(Linear): Epoch 80/100, Loss: 0.00001\n",
            "****CVX method****(Linear): Epoch 80/100, Loss: 0.00078\n",
            "****FTN method****(Linear): Epoch 81/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 81/100, Loss: 0.00075\n",
            "****FTN method****(Linear): Epoch 82/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 82/100, Loss: 0.00071\n",
            "****FTN method****(Linear): Epoch 83/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 83/100, Loss: 0.00069\n",
            "****FTN method****(Linear): Epoch 84/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 84/100, Loss: 0.00066\n",
            "****FTN method****(Linear): Epoch 85/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 85/100, Loss: 0.00064\n",
            "****FTN method****(Linear): Epoch 86/100, Loss: 0.00001\n",
            "****CVX method****(Linear): Epoch 86/100, Loss: 0.00061\n",
            "****FTN method****(Linear): Epoch 87/100, Loss: 0.00004\n",
            "****CVX method****(Linear): Epoch 87/100, Loss: 0.00059\n",
            "****FTN method****(Linear): Epoch 88/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 88/100, Loss: 0.00057\n",
            "****FTN method****(Linear): Epoch 89/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 89/100, Loss: 0.00055\n",
            "****FTN method****(Linear): Epoch 90/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 90/100, Loss: 0.00053\n",
            "****FTN method****(Linear): Epoch 91/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 91/100, Loss: 0.00051\n",
            "****FTN method****(Linear): Epoch 92/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 92/100, Loss: 0.00049\n",
            "****FTN method****(Linear): Epoch 93/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 93/100, Loss: 0.00047\n",
            "****FTN method****(Linear): Epoch 94/100, Loss: 0.00005\n",
            "****CVX method****(Linear): Epoch 94/100, Loss: 0.00045\n",
            "****FTN method****(Linear): Epoch 95/100, Loss: 0.00001\n",
            "****CVX method****(Linear): Epoch 95/100, Loss: 0.00044\n",
            "****FTN method****(Linear): Epoch 96/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 96/100, Loss: 0.00042\n",
            "****FTN method****(Linear): Epoch 97/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 97/100, Loss: 0.00041\n",
            "****FTN method****(Linear): Epoch 98/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 98/100, Loss: 0.00039\n",
            "****FTN method****(Linear): Epoch 99/100, Loss: 0.00003\n",
            "****CVX method****(Linear): Epoch 99/100, Loss: 0.00038\n",
            "****FTN method****(Linear): Epoch 100/100, Loss: 0.00002\n",
            "****CVX method****(Linear): Epoch 100/100, Loss: 0.00037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the performance"
      ],
      "metadata": {
        "id": "22bTT1CiZmgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "q_test = (np.random.rand(12,1000).astype(np.float32) - 0.5)*0.2 #range [-0.1,0.1]\n",
        "v_test = X@q_test + 1"
      ],
      "metadata": {
        "id": "v5AlIFr1Xke9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_v = FTN_net(torch.tensor(q_test.T))\n",
        "cvx_predicted_v = cvx_net(torch.tensor(q_test.T))"
      ],
      "metadata": {
        "id": "hKdvapUTZzot"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the mean square error\n",
        "print(np.mean(np.square(predicted_v.detach().numpy() - v_test.T)))\n",
        "print(np.mean(np.square(cvx_predicted_v.detach().numpy() - v_test.T)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgjM-PqLbOer",
        "outputId": "16ea70da-e64e-4a75-d23d-9dc78b08dbfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3519971514479501e-05\n",
            "0.00035759874583684176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test with nonlinear power system"
      ],
      "metadata": {
        "id": "P3621wRccE8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandapower"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_P8XFNpbVVH",
        "outputId": "bafa90fe-261d-4b97-9f92-118dc7cbef84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandapower in /usr/local/lib/python3.10/dist-packages (2.14.9)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from pandapower) (2.0.3)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from pandapower) (3.3)\n",
            "Requirement already satisfied: scipy<1.14 in /usr/local/lib/python3.10/dist-packages (from pandapower) (1.11.4)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pandapower) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pandapower) (24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pandapower) (4.66.4)\n",
            "Requirement already satisfied: deepdiff in /usr/local/lib/python3.10/dist-packages (from pandapower) (7.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->pandapower) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->pandapower) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->pandapower) (2024.1)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff->pandapower) (4.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->pandapower) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import pandapower as pp\n",
        "import pandapower.networks as pn"
      ],
      "metadata": {
        "id": "VwDUTgDzcPTG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "def create_13bus():\n",
        "    pp_net = pp.converter.from_mpc('case_13.mat', casename_mpc_file='case_mpc')\n",
        "\n",
        "    pp_net.sgen['p_mw'] = 0.0\n",
        "    pp_net.sgen['q_mvar'] = 0.0\n",
        "\n",
        "    pp.create_sgen(pp_net, 2, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 7, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 9, p_mw = 0, q_mvar=0)\n",
        "\n",
        "    pp.create_sgen(pp_net, 1, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 3, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 4, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 5, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 6, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 8, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 10, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 11, p_mw = 0, q_mvar=0)\n",
        "    pp.create_sgen(pp_net, 12, p_mw = 0, q_mvar=0)\n",
        "\n",
        "    # In the original IEEE 13 bus system, there is no load in bus 3, 7, 8.\n",
        "    # Add the load to corresponding node for dimension alignment in RL training\n",
        "    pp.create_load(pp_net, 3, p_mw = 0, q_mvar=0)\n",
        "    pp.create_load(pp_net, 7, p_mw = 0, q_mvar=0)\n",
        "    pp.create_load(pp_net, 8, p_mw = 0, q_mvar=0)\n",
        "\n",
        "    return pp_net\n",
        "\n",
        "\n",
        "class IEEE13bus(gym.Env):\n",
        "    def __init__(self, pp_net, injection_bus, v0=1, vmax=1.05, vmin=0.95, all_bus=False):\n",
        "        self.network =  pp_net\n",
        "        self.obs_dim = 1\n",
        "        self.action_dim = 1\n",
        "        self.injection_bus = injection_bus\n",
        "        self.agentnum = len(injection_bus)\n",
        "        # if self.agentnum == 12:  # comment out for mpc experiments\n",
        "        #     all_bus=True\n",
        "        self.v0 = v0\n",
        "        self.vmax = vmax\n",
        "        self.vmin = vmin\n",
        "\n",
        "        self.load0_p = np.copy(self.network.load['p_mw'])\n",
        "        self.load0_q = np.copy(self.network.load['q_mvar'])\n",
        "\n",
        "        self.gen0_p = np.copy(self.network.sgen['p_mw'])\n",
        "        self.gen0_q = np.copy(self.network.sgen['q_mvar'])\n",
        "        self.all_bus = all_bus\n",
        "\n",
        "        self.state = np.ones(self.agentnum, )\n",
        "\n",
        "    def step(self, action):\n",
        "        # state-transition dynamics\n",
        "        for i in range(len(self.injection_bus)):\n",
        "            self.network.sgen.at[i, 'q_mvar'] = action[i]\n",
        "\n",
        "        pp.runpp(self.network, algorithm='bfsw', init = 'dc')\n",
        "\n",
        "        self.state = self.network.res_bus.iloc[self.injection_bus].vm_pu.to_numpy()\n",
        "        return self.state\n",
        "\n",
        "    def reset0(self, seed=1): #reset voltage to nominal value\n",
        "\n",
        "        self.network.load['p_mw'] = 0*self.load0_p\n",
        "        self.network.load['q_mvar'] = 0*self.load0_q\n",
        "\n",
        "        self.network.sgen['p_mw'] = 0*self.gen0_p\n",
        "        self.network.sgen['q_mvar'] = 0*self.gen0_q\n",
        "\n",
        "        pp.runpp(self.network, algorithm='bfsw')\n",
        "        self.state = self.network.res_bus.iloc[self.injection_bus].vm_pu.to_numpy()\n",
        "        return self.state\n",
        "\n"
      ],
      "metadata": {
        "id": "nRPh7f8ycVnG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = create_13bus()\n",
        "injection_bus = np.array([1,2,5,3,4,7,8,9,6,10,11,12])\n",
        "env = IEEE13bus(net, injection_bus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCqdvWtAdwcj",
        "outputId": "7ed70f8e-ed2f-426a-d40b-ec5e94ebe208"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate data\n",
        "v_list = []\n",
        "q_list = []\n",
        "env.reset0()\n",
        "for i in range(10000):\n",
        "  q = (np.random.rand(12).astype(np.float32) - 0.5)*0.4 #range [-0.1,0.1]\n",
        "  env.step(q)\n",
        "  v_list.append(env.state)\n",
        "  q_list.append(q)\n",
        "v_data = np.array(v_list)\n",
        "q_data = np.array(q_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lew1mzbenkT",
        "outputId": "62577cec-bdf0-4c84-ccea-73bd50802cae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 100\n",
        "v_data = np.float32(v_data)\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(q_data), torch.tensor(v_data))\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "FTN_net, FTN_optimizer, cvx_net,cvx_optimizer,criterion = init_NNs()\n",
        "\n",
        "loss_cvx_list = []\n",
        "loss_ftn_list = []\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    cvx_epoch_loss = 0\n",
        "    for i, (q, v) in enumerate(train_dataloader):\n",
        "        # Reset gradients\n",
        "        FTN_optimizer.zero_grad()\n",
        "        cvx_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute predicted actions by passing states to the model\n",
        "        predicted_v = FTN_net(q)\n",
        "        cvx_predicted_v = cvx_net(q)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(predicted_v, v)\n",
        "        cvx_loss = criterion(cvx_predicted_v, v)\n",
        "\n",
        "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        cvx_loss.backward()\n",
        "\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        FTN_optimizer.step()\n",
        "        cvx_optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        epoch_loss += loss.item()\n",
        "        cvx_epoch_loss += cvx_loss.item()\n",
        "    loss_cvx_list.append(cvx_epoch_loss)\n",
        "    loss_ftn_list.append(epoch_loss)\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f\"****FTN method****(Linear): Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/i:.5f}\")\n",
        "    print(f\"****CVX method****(Linear): Epoch {epoch+1}/{epochs}, Loss: {cvx_epoch_loss/i:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEhUz7XNe8UR",
        "outputId": "9c9e0fef-925d-4e31-e5a3-3776a2eaf986"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****FTN method****(Linear): Epoch 1/100, Loss: 0.51592\n",
            "****CVX method****(Linear): Epoch 1/100, Loss: 0.97385\n",
            "****FTN method****(Linear): Epoch 2/100, Loss: 0.13363\n",
            "****CVX method****(Linear): Epoch 2/100, Loss: 0.34439\n",
            "****FTN method****(Linear): Epoch 3/100, Loss: 0.03372\n",
            "****CVX method****(Linear): Epoch 3/100, Loss: 0.15213\n",
            "****FTN method****(Linear): Epoch 4/100, Loss: 0.01463\n",
            "****CVX method****(Linear): Epoch 4/100, Loss: 0.11556\n",
            "****FTN method****(Linear): Epoch 5/100, Loss: 0.00833\n",
            "****CVX method****(Linear): Epoch 5/100, Loss: 0.10291\n",
            "****FTN method****(Linear): Epoch 6/100, Loss: 0.00505\n",
            "****CVX method****(Linear): Epoch 6/100, Loss: 0.09239\n",
            "****FTN method****(Linear): Epoch 7/100, Loss: 0.00330\n",
            "****CVX method****(Linear): Epoch 7/100, Loss: 0.08267\n",
            "****FTN method****(Linear): Epoch 8/100, Loss: 0.00225\n",
            "****CVX method****(Linear): Epoch 8/100, Loss: 0.07376\n",
            "****FTN method****(Linear): Epoch 9/100, Loss: 0.00163\n",
            "****CVX method****(Linear): Epoch 9/100, Loss: 0.06568\n",
            "****FTN method****(Linear): Epoch 10/100, Loss: 0.00118\n",
            "****CVX method****(Linear): Epoch 10/100, Loss: 0.05886\n",
            "****FTN method****(Linear): Epoch 11/100, Loss: 0.00095\n",
            "****CVX method****(Linear): Epoch 11/100, Loss: 0.05252\n",
            "****FTN method****(Linear): Epoch 12/100, Loss: 0.00070\n",
            "****CVX method****(Linear): Epoch 12/100, Loss: 0.04705\n",
            "****FTN method****(Linear): Epoch 13/100, Loss: 0.00055\n",
            "****CVX method****(Linear): Epoch 13/100, Loss: 0.04219\n",
            "****FTN method****(Linear): Epoch 14/100, Loss: 0.00047\n",
            "****CVX method****(Linear): Epoch 14/100, Loss: 0.03781\n",
            "****FTN method****(Linear): Epoch 15/100, Loss: 0.00038\n",
            "****CVX method****(Linear): Epoch 15/100, Loss: 0.03407\n",
            "****FTN method****(Linear): Epoch 16/100, Loss: 0.00036\n",
            "****CVX method****(Linear): Epoch 16/100, Loss: 0.03058\n",
            "****FTN method****(Linear): Epoch 17/100, Loss: 0.00027\n",
            "****CVX method****(Linear): Epoch 17/100, Loss: 0.02758\n",
            "****FTN method****(Linear): Epoch 18/100, Loss: 0.00025\n",
            "****CVX method****(Linear): Epoch 18/100, Loss: 0.02495\n",
            "****FTN method****(Linear): Epoch 19/100, Loss: 0.00022\n",
            "****CVX method****(Linear): Epoch 19/100, Loss: 0.02261\n",
            "****FTN method****(Linear): Epoch 20/100, Loss: 0.00022\n",
            "****CVX method****(Linear): Epoch 20/100, Loss: 0.02065\n",
            "****FTN method****(Linear): Epoch 21/100, Loss: 0.00020\n",
            "****CVX method****(Linear): Epoch 21/100, Loss: 0.01874\n",
            "****FTN method****(Linear): Epoch 22/100, Loss: 0.00019\n",
            "****CVX method****(Linear): Epoch 22/100, Loss: 0.01711\n",
            "****FTN method****(Linear): Epoch 23/100, Loss: 0.00017\n",
            "****CVX method****(Linear): Epoch 23/100, Loss: 0.01562\n",
            "****FTN method****(Linear): Epoch 24/100, Loss: 0.00019\n",
            "****CVX method****(Linear): Epoch 24/100, Loss: 0.01434\n",
            "****FTN method****(Linear): Epoch 25/100, Loss: 0.00014\n",
            "****CVX method****(Linear): Epoch 25/100, Loss: 0.01318\n",
            "****FTN method****(Linear): Epoch 26/100, Loss: 0.00018\n",
            "****CVX method****(Linear): Epoch 26/100, Loss: 0.01208\n",
            "****FTN method****(Linear): Epoch 27/100, Loss: 0.00032\n",
            "****CVX method****(Linear): Epoch 27/100, Loss: 0.01110\n",
            "****FTN method****(Linear): Epoch 28/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 28/100, Loss: 0.01030\n",
            "****FTN method****(Linear): Epoch 29/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 29/100, Loss: 0.00948\n",
            "****FTN method****(Linear): Epoch 30/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 30/100, Loss: 0.00877\n",
            "****FTN method****(Linear): Epoch 31/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 31/100, Loss: 0.00814\n",
            "****FTN method****(Linear): Epoch 32/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 32/100, Loss: 0.00755\n",
            "****FTN method****(Linear): Epoch 33/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 33/100, Loss: 0.00705\n",
            "****FTN method****(Linear): Epoch 34/100, Loss: 0.00015\n",
            "****CVX method****(Linear): Epoch 34/100, Loss: 0.00654\n",
            "****FTN method****(Linear): Epoch 35/100, Loss: 0.00015\n",
            "****CVX method****(Linear): Epoch 35/100, Loss: 0.00611\n",
            "****FTN method****(Linear): Epoch 36/100, Loss: 0.00014\n",
            "****CVX method****(Linear): Epoch 36/100, Loss: 0.00570\n",
            "****FTN method****(Linear): Epoch 37/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 37/100, Loss: 0.00533\n",
            "****FTN method****(Linear): Epoch 38/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 38/100, Loss: 0.00498\n",
            "****FTN method****(Linear): Epoch 39/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 39/100, Loss: 0.00467\n",
            "****FTN method****(Linear): Epoch 40/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 40/100, Loss: 0.00437\n",
            "****FTN method****(Linear): Epoch 41/100, Loss: 0.00015\n",
            "****CVX method****(Linear): Epoch 41/100, Loss: 0.00410\n",
            "****FTN method****(Linear): Epoch 42/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 42/100, Loss: 0.00385\n",
            "****FTN method****(Linear): Epoch 43/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 43/100, Loss: 0.00363\n",
            "****FTN method****(Linear): Epoch 44/100, Loss: 0.00016\n",
            "****CVX method****(Linear): Epoch 44/100, Loss: 0.00342\n",
            "****FTN method****(Linear): Epoch 45/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 45/100, Loss: 0.00322\n",
            "****FTN method****(Linear): Epoch 46/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 46/100, Loss: 0.00304\n",
            "****FTN method****(Linear): Epoch 47/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 47/100, Loss: 0.00288\n",
            "****FTN method****(Linear): Epoch 48/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 48/100, Loss: 0.00272\n",
            "****FTN method****(Linear): Epoch 49/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 49/100, Loss: 0.00258\n",
            "****FTN method****(Linear): Epoch 50/100, Loss: 0.00014\n",
            "****CVX method****(Linear): Epoch 50/100, Loss: 0.00244\n",
            "****FTN method****(Linear): Epoch 51/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 51/100, Loss: 0.00231\n",
            "****FTN method****(Linear): Epoch 52/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 52/100, Loss: 0.00219\n",
            "****FTN method****(Linear): Epoch 53/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 53/100, Loss: 0.00208\n",
            "****FTN method****(Linear): Epoch 54/100, Loss: 0.00017\n",
            "****CVX method****(Linear): Epoch 54/100, Loss: 0.00197\n",
            "****FTN method****(Linear): Epoch 55/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 55/100, Loss: 0.00188\n",
            "****FTN method****(Linear): Epoch 56/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 56/100, Loss: 0.00178\n",
            "****FTN method****(Linear): Epoch 57/100, Loss: 0.00017\n",
            "****CVX method****(Linear): Epoch 57/100, Loss: 0.00170\n",
            "****FTN method****(Linear): Epoch 58/100, Loss: 0.00014\n",
            "****CVX method****(Linear): Epoch 58/100, Loss: 0.00162\n",
            "****FTN method****(Linear): Epoch 59/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 59/100, Loss: 0.00153\n",
            "****FTN method****(Linear): Epoch 60/100, Loss: 0.00008\n",
            "****CVX method****(Linear): Epoch 60/100, Loss: 0.00147\n",
            "****FTN method****(Linear): Epoch 61/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 61/100, Loss: 0.00140\n",
            "****FTN method****(Linear): Epoch 62/100, Loss: 0.00014\n",
            "****CVX method****(Linear): Epoch 62/100, Loss: 0.00133\n",
            "****FTN method****(Linear): Epoch 63/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 63/100, Loss: 0.00127\n",
            "****FTN method****(Linear): Epoch 64/100, Loss: 0.00013\n",
            "****CVX method****(Linear): Epoch 64/100, Loss: 0.00122\n",
            "****FTN method****(Linear): Epoch 65/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 65/100, Loss: 0.00117\n",
            "****FTN method****(Linear): Epoch 66/100, Loss: 0.00017\n",
            "****CVX method****(Linear): Epoch 66/100, Loss: 0.00111\n",
            "****FTN method****(Linear): Epoch 67/100, Loss: 0.00008\n",
            "****CVX method****(Linear): Epoch 67/100, Loss: 0.00106\n",
            "****FTN method****(Linear): Epoch 68/100, Loss: 0.00011\n",
            "****CVX method****(Linear): Epoch 68/100, Loss: 0.00102\n",
            "****FTN method****(Linear): Epoch 69/100, Loss: 0.00016\n",
            "****CVX method****(Linear): Epoch 69/100, Loss: 0.00098\n",
            "****FTN method****(Linear): Epoch 70/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 70/100, Loss: 0.00094\n",
            "****FTN method****(Linear): Epoch 71/100, Loss: 0.00008\n",
            "****CVX method****(Linear): Epoch 71/100, Loss: 0.00090\n",
            "****FTN method****(Linear): Epoch 72/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 72/100, Loss: 0.00086\n",
            "****FTN method****(Linear): Epoch 73/100, Loss: 0.00006\n",
            "****CVX method****(Linear): Epoch 73/100, Loss: 0.00083\n",
            "****FTN method****(Linear): Epoch 74/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 74/100, Loss: 0.00079\n",
            "****FTN method****(Linear): Epoch 75/100, Loss: 0.00017\n",
            "****CVX method****(Linear): Epoch 75/100, Loss: 0.00076\n",
            "****FTN method****(Linear): Epoch 76/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 76/100, Loss: 0.00073\n",
            "****FTN method****(Linear): Epoch 77/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 77/100, Loss: 0.00070\n",
            "****FTN method****(Linear): Epoch 78/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 78/100, Loss: 0.00067\n",
            "****FTN method****(Linear): Epoch 79/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 79/100, Loss: 0.00065\n",
            "****FTN method****(Linear): Epoch 80/100, Loss: 0.00012\n",
            "****CVX method****(Linear): Epoch 80/100, Loss: 0.00063\n",
            "****FTN method****(Linear): Epoch 81/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 81/100, Loss: 0.00060\n",
            "****FTN method****(Linear): Epoch 82/100, Loss: 0.00022\n",
            "****CVX method****(Linear): Epoch 82/100, Loss: 0.00058\n",
            "****FTN method****(Linear): Epoch 83/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 83/100, Loss: 0.00056\n",
            "****FTN method****(Linear): Epoch 84/100, Loss: 0.00006\n",
            "****CVX method****(Linear): Epoch 84/100, Loss: 0.00054\n",
            "****FTN method****(Linear): Epoch 85/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 85/100, Loss: 0.00052\n",
            "****FTN method****(Linear): Epoch 86/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 86/100, Loss: 0.00050\n",
            "****FTN method****(Linear): Epoch 87/100, Loss: 0.00014\n",
            "****CVX method****(Linear): Epoch 87/100, Loss: 0.00048\n",
            "****FTN method****(Linear): Epoch 88/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 88/100, Loss: 0.00047\n",
            "****FTN method****(Linear): Epoch 89/100, Loss: 0.00015\n",
            "****CVX method****(Linear): Epoch 89/100, Loss: 0.00045\n",
            "****FTN method****(Linear): Epoch 90/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 90/100, Loss: 0.00044\n",
            "****FTN method****(Linear): Epoch 91/100, Loss: 0.00010\n",
            "****CVX method****(Linear): Epoch 91/100, Loss: 0.00042\n",
            "****FTN method****(Linear): Epoch 92/100, Loss: 0.00006\n",
            "****CVX method****(Linear): Epoch 92/100, Loss: 0.00041\n",
            "****FTN method****(Linear): Epoch 93/100, Loss: 0.00017\n",
            "****CVX method****(Linear): Epoch 93/100, Loss: 0.00039\n",
            "****FTN method****(Linear): Epoch 94/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 94/100, Loss: 0.00038\n",
            "****FTN method****(Linear): Epoch 95/100, Loss: 0.00008\n",
            "****CVX method****(Linear): Epoch 95/100, Loss: 0.00037\n",
            "****FTN method****(Linear): Epoch 96/100, Loss: 0.00015\n",
            "****CVX method****(Linear): Epoch 96/100, Loss: 0.00036\n",
            "****FTN method****(Linear): Epoch 97/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 97/100, Loss: 0.00034\n",
            "****FTN method****(Linear): Epoch 98/100, Loss: 0.00007\n",
            "****CVX method****(Linear): Epoch 98/100, Loss: 0.00033\n",
            "****FTN method****(Linear): Epoch 99/100, Loss: 0.00019\n",
            "****CVX method****(Linear): Epoch 99/100, Loss: 0.00032\n",
            "****FTN method****(Linear): Epoch 100/100, Loss: 0.00009\n",
            "****CVX method****(Linear): Epoch 100/100, Loss: 0.00031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "v_list = []\n",
        "q_list = []\n",
        "env.reset0()\n",
        "for i in range(1000):\n",
        "  q = (np.random.rand(12).astype(np.float32) - 0.5)*0.4 #range [-0.1,0.1]\n",
        "  env.step(q)\n",
        "  v_list.append(env.state)\n",
        "  q_list.append(q)\n",
        "v_test = np.array(v_list)\n",
        "q_test = np.array(q_list)"
      ],
      "metadata": {
        "id": "vjLOwpVgkqB7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_v = FTN_net(torch.tensor(q_test))\n",
        "cvx_predicted_v = cvx_net(torch.tensor(q_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPssb-dAk3QY",
        "outputId": "a1231767-6bce-4d19-ca2c-1d73327153fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the mean square error\n",
        "print(np.mean(np.square(predicted_v.detach().numpy() - v_test)))\n",
        "print(np.mean(np.square(cvx_predicted_v.detach().numpy() - v_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuQjkMxtlBVc",
        "outputId": "57755ec8-9501-4e6d-a34b-8f912010b41e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.103201279504628e-05\n",
            "0.0003054466196824061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2L6RAo3lCnU"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}